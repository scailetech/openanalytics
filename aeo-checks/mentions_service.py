"""AEO Mentions Check Service - AI Platform Visibility Analysis

Queries multiple AI platforms (via OpenRouter) to check company visibility:
- Perplexity (sonar-pro) - native search
- Claude (claude-3.5-sonnet) - with google_search tool
- ChatGPT (openai/gpt-4.1) - with google_search tool
- Gemini (gemini-3-pro-preview) - with google_search tool

All platforms use our DataForSEO SERP via scaile-services google_search tool.

Features:
- Quality-adjusted scoring with mention capping (max 3 per response)
- Position detection (#1 in list gets boost)
- Dimension-based query generation (Branded, Service-Specific, etc.)
- Fast mode (10 queries, Gemini + ChatGPT only) vs Full mode (50 queries, all platforms)

v4: GPT-4.1 for ChatGPT, DataForSEO SERP for all platforms
"""

import os
import re
import asyncio
import logging
from typing import Optional, List, Dict, Any
from datetime import datetime

import httpx
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from ai_client import AIClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="AEO Mentions Check Service",
    description="AI platform visibility analysis with quality-adjusted scoring. Uses DataForSEO SERP via local tool.",
    version="4.1.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Local AI Client (lazy initialization)
_ai_client = None

def get_ai_client():
    """Get AI client instance (lazy initialization)."""
    global _ai_client
    if _ai_client is None:
        _ai_client = AIClient()
    return _ai_client

# AI Platforms with search capabilities (via OpenRouter + DataForSEO SERP)
# All platforms use google_search tool which routes to our DataForSEO SERP
AI_PLATFORMS = {
    "perplexity": {
        "model": "perplexity/sonar-pro",
        "has_search": True,  # Native search built-in
        "needs_tool": False,  # Perplexity has native web search
        "provider": None,
    },
    "claude": {
        "model": "anthropic/claude-3.5-sonnet",
        "has_search": True,
        "needs_tool": True,  # Uses google_search tool → DataForSEO
        "provider": None,
    },
    "chatgpt": {
        "model": "openai/gpt-4.1",  # GPT-4.1 (newer, better reasoning)
        "has_search": True,
        "needs_tool": True,  # Uses google_search tool → DataForSEO
        "provider": "openai",  # Force OpenAI provider (Azure requires BYOK)
    },
    "gemini": {
        "model": "google/gemini-3-pro-preview",
        "has_search": True,
        "needs_tool": True,  # Uses google_search tool → DataForSEO
        "provider": None,
    },
}


# ==================== Request/Response Models ====================

class CompanyAnalysis(BaseModel):
    """Company analysis data for generating targeted queries."""
    companyInfo: Dict[str, Any] = Field(default_factory=dict)
    competitors: List[Dict[str, Any]] = Field(default_factory=list)


class MentionsCheckRequest(BaseModel):
    companyName: str
    companyAnalysis: CompanyAnalysis = Field(
        ...,  # Required field
        description="Company analysis data (required for targeted query generation)"
    )
    language: str = "english"
    country: str = "US"
    numQueries: int = 50
    mode: str = Field(default="full", description="'full' (50 queries, all platforms) or 'fast' (10 queries, Gemini + ChatGPT only)")
    generateInsights: bool = False
    platforms: Optional[List[str]] = None  # If None, use all platforms


class QueryResult(BaseModel):
    query: str
    dimension: str
    platform: str
    raw_mentions: int
    capped_mentions: int
    quality_score: float
    mention_type: str
    position: Optional[int] = None
    source_urls: List[str] = Field(default_factory=list)
    competitor_mentions: List[Dict[str, Any]] = Field(default_factory=list)
    response_text: str = ""


class PlatformStats(BaseModel):
    mentions: int
    quality_score: float
    responses: int
    errors: int
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    cost: float = 0.0


class DimensionStats(BaseModel):
    mentions: int
    quality_score: float
    queries: int


class MentionsCheckResponse(BaseModel):
    companyName: str
    visibility: float  # Presence-based visibility score (0-100%)
    band: str  # Dominant/Strong/Moderate/Weak/Minimal
    mentions: int  # Total capped mentions across all responses
    presence_rate: float  # What % of responses mentioned the company (0-100%)
    quality_score: float  # Average quality score when mentioned (0-10)
    max_quality: float  # Maximum possible quality (responses × 10)
    platform_stats: Dict[str, PlatformStats]
    dimension_stats: Dict[str, DimensionStats]
    query_results: List[QueryResult]
    actualQueriesProcessed: int
    execution_time_seconds: float
    total_cost: float
    total_tokens: int
    mode: str


# ==================== Quality Scoring Functions ====================

def detect_mention_type(text: str, company_name: str) -> str:
    """Detect mention quality type."""
    text_lower = text.lower()
    company_lower = company_name.lower()

    recommend_patterns = [
        f"recommend {company_lower}",
        f"{company_lower} is the best",
        f"best.*{company_lower}",
        f"{company_lower}.*excellent",
        f"top choice.*{company_lower}"
    ]

    for pattern in recommend_patterns:
        if re.search(pattern, text_lower):
            return 'primary_recommendation'

    if re.search(f"(top|leading|best).*{company_lower}", text_lower):
        return 'top_option'

    if re.search(f"\\d+\\.|\\*.*{company_lower}", text):
        return 'listed_option'

    if company_lower in text_lower:
        return 'mentioned_in_context'

    return 'none'


def detect_list_position(text: str, company_name: str) -> Optional[int]:
    """Detect position in numbered/bulleted lists."""
    lines = text.split('\n')
    for i, line in enumerate(lines):
        if re.search(re.escape(company_name), line, re.IGNORECASE):
            match = re.match(r'^\s*([\d]+)[\.)\s]', line)
            if match:
                return int(match.group(1))
            if re.match(r'^\s*[\*\-\•]', line):
                return i + 1
    return None


def count_mentions(text: str, company_name: str) -> Dict[str, Any]:
    """Count and cap mentions with quality scoring.
    
    Scoring Philosophy:
    - Being mentioned at ALL is valuable (base score)
    - HOW you're mentioned adds bonus points
    - Position in lists adds additional bonus
    - Max score is 10 per response
    
    This produces intuitive visibility percentages:
    - Dominant (70%+): Consistently recommended as top choice
    - Strong (50-70%): Frequently mentioned as top option
    - Moderate (30-50%): Regularly appears in lists
    - Weak (10-30%): Occasionally mentioned
    - Minimal (<10%): Rarely or never mentioned
    """
    raw_mentions = len(re.findall(re.escape(company_name), text, re.IGNORECASE))
    
    if raw_mentions == 0:
        return {
            'raw_mentions': 0,
            'capped_mentions': 0,
            'quality_score': 0.0,
            'mention_type': 'none',
            'position': None,
        }

    capped_mentions = min(raw_mentions, 3)  # Cap at 3
    mention_type = detect_mention_type(text, company_name)
    position = detect_list_position(text, company_name)

    # Base scores by mention type (how valuable is this type of mention?)
    # These are the PRIMARY component of the score
    base_scores = {
        'primary_recommendation': 9.0,   # "I recommend X" - highest value
        'top_option': 7.0,               # "top/leading/best X" - high value
        'listed_option': 5.0,            # Listed among options - medium value
        'mentioned_in_context': 3.0,     # Just mentioned - still valuable
        'none': 0.0,                     # Not mentioned - no value
    }
    base_score = base_scores.get(mention_type, 3.0)

    # Position bonus (additive - rewards being listed first)
    position_bonus = 0.0
    if position:
        if position == 1:
            position_bonus = 2.0   # #1 position is valuable
        elif position <= 3:
            position_bonus = 1.0   # Top 3 is good
        elif position <= 5:
            position_bonus = 0.5   # Top 5 is okay
        # 6+ gets no bonus

    # Multiple mentions bonus (small additive bonus for repeated mentions)
    mention_bonus = min(1.0, (capped_mentions - 1) * 0.5)  # 0, 0.5, or 1.0

    quality_score = min(10.0, base_score + position_bonus + mention_bonus)

    return {
        'raw_mentions': raw_mentions,
        'capped_mentions': capped_mentions,
        'quality_score': round(quality_score, 2),
        'mention_type': mention_type,
        'position': position,
    }


def extract_competitor_mentions(text: str, competitors: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Extract competitor mentions from response."""
    results = []
    for comp in competitors:
        name = comp.get("name", "")
        if name:
            count = len(re.findall(re.escape(name), text, re.IGNORECASE))
            if count > 0:
                results.append({"name": name, "count": count})
    return results


# ==================== Query Generation ====================

def generate_queries(
    company_name: str,
    company_analysis: Optional[CompanyAnalysis],
    num_queries: int,
    mode: str,
) -> List[Dict[str, str]]:
    """Generate test queries across dimensions."""
    queries = []
    
    # Always include branded queries
    queries.append({"query": company_name, "dimension": "Branded"})
    queries.append({"query": f"{company_name} software", "dimension": "Branded"})
    
    # Extract info from company analysis
    industry = ""
    product_category = ""
    services = []
    pain_points = []
    
    # Extract geography data (use what frontend sends)
    country_specific_queries = []
    geographic_modifiers = []
    regulatory_requirements = []
    use_cases = []
    
    if company_analysis and company_analysis.companyInfo:
        info = company_analysis.companyInfo
        industry = info.get("industry", "")
        product_category = info.get("productCategory", "")
        services = info.get("services", []) or []
        pain_points = info.get("pain_points", []) or []
        
        # Extract geography data
        country_specific_queries = info.get("country_specific_queries", []) or []
        geographic_modifiers = info.get("geographic_modifiers", []) or []
        regulatory_requirements = info.get("regulatory_requirements", []) or []
        use_cases = info.get("use_cases", []) or []
    
    # AI-platform + geography queries (highest priority for AEO)
    ai_platforms = ["ChatGPT", "Perplexity", "Claude", "Gemini"]
    if pain_points and geographic_modifiers:
        for pain_point in pain_points[:2]:
            for platform in ai_platforms[:2]:  # Top 2 platforms
                for geo_mod in geographic_modifiers[:1]:  # Top geo modifier
                    queries.append({"query": f"how to {pain_point} with {platform} for {geo_mod} companies", "dimension": "AI-Platform-Geography"})
    
    # Service + geography combinations for AEO-specific targeting
    if services and geographic_modifiers:
        for service in services[:1]:
            for geo_mod in geographic_modifiers[:1]:
                queries.append({"query": f"{service} for {geo_mod} enterprises", "dimension": "Service-Geography"})
    
    # Industry + geography combinations for hyperniche targeting
    if industry and geographic_modifiers:
        for geo_mod in geographic_modifiers[:1]:
            queries.append({"query": f"best {industry} for {geo_mod} companies", "dimension": "Industry-Geography"})
            if pain_points:
                # Industry + geography + pain point (ultra-specific)
                pain_point = pain_points[0]
                queries.append({"query": f"how to {pain_point} in {industry} for {geo_mod} enterprises", "dimension": "Industry-Geography-Intent"})
    
    # Use pre-generated use cases (already hyperniche)
    if use_cases:
        for use_case in use_cases[:2]:
            queries.append({"query": f"best practices for {use_case}", "dimension": "Use-Case/Intent"})
    
    # Compliance + AI platform queries  
    if regulatory_requirements and services:
        for req in regulatory_requirements[:1]:
            for service in services[:1]:
                queries.append({"query": f"{service} with {req} compliance", "dimension": "Compliance-Focused"})
    
    # Service-specific queries
    if services:
        for service in services[:2]:
            queries.append({"query": f"{service} software", "dimension": "Service-Specific"})
    elif industry:
        queries.append({"query": f"{industry} software", "dimension": "Service-Specific"})
    
    # Industry/vertical queries
    if industry:
        queries.append({"query": f"best {industry} tools", "dimension": "Industry/Vertical"})
        queries.append({"query": f"{industry} solutions", "dimension": "Industry/Vertical"})
    
    # Use-case queries
    if pain_points:
        for pain_point in pain_points[:2]:
            queries.append({"query": f"how to {pain_point}", "dimension": "Use-Case/Intent"})
    
    # Competitive queries
    queries.append({"query": f"{company_name} vs alternatives", "dimension": "Competitive"})
    queries.append({"query": f"{company_name} competitors", "dimension": "Competitive"})
    
    # Broad category (lowest priority - generic queries)
    if product_category:
        queries.append({"query": f"best {product_category}", "dimension": "Broad Category"})
    # Generic fallback query (moved to last priority)
    current_year = datetime.now().year
    queries.append({"query": f"best software tools {current_year}", "dimension": "Broad Category"})
    
    # Limit based on mode
    if mode == "fast":
        return queries[:10]  # Fast mode: 10 queries
    return queries[:num_queries]  # Full mode: 50 queries


# ==================== AI Platform Queries ====================

async def query_platform(
    platform: str,
    query: str,
    model_config: Dict[str, Any],
) -> Dict[str, Any]:
    """Query a single AI platform via local AIClient."""
    model = model_config["model"]
    needs_tool = model_config.get("needs_tool", False)
    provider = model_config.get("provider")
    
    try:
        # Use google_search tool for platforms that need it
        tools = ["google_search"] if needs_tool else None
        
        # Call AI locally
        messages = [{"role": "user", "content": query}]
        
        result = await get_ai_client().complete_with_tools(
            messages=messages,
            model=model,
            tools=tools,
            temperature=0.7,
            max_tokens=1024,
            provider=provider
        )
        
        # Extract response
        choice = result.get("choices", [{}])[0]
        content = choice.get("message", {}).get("content", "")
        
        return {
            "platform": platform,
            "response": content,
            "tokens": result.get("usage", {}).get("total_tokens", 0),
            "cost": 0.0,
        }
            
    except Exception as e:
        logger.error(f"{platform} query error: {e}")
        return {"error": str(e), "platform": platform}


async def query_all_platforms(
    query: str,
    platforms: List[str],
) -> List[Dict[str, Any]]:
    """Query all platforms in parallel."""
    tasks = []
    for platform in platforms:
        if platform in AI_PLATFORMS:
            tasks.append(query_platform(platform, query, AI_PLATFORMS[platform]))
    
    return await asyncio.gather(*tasks, return_exceptions=True)


# ==================== API Endpoints ====================

@app.post("/check", response_model=MentionsCheckResponse)
async def check_mentions(request: MentionsCheckRequest):
    """Run AEO mentions check across AI platforms.
    
    Requires companyAnalysis with industry or products data for targeted query generation.
    Without this data, queries would be too generic to produce meaningful visibility scores.
    """
    import time
    start_time = time.time()
    
    # Validate companyAnalysis has REAL data from company analysis (not just CSV data)
    # This is STRICT validation - we require products/services from actual analysis
    company_info = request.companyAnalysis.companyInfo if request.companyAnalysis else {}
    
    # Get the actual data
    products = company_info.get("products") or []
    services = company_info.get("services") or []
    industry = company_info.get("industry", "")
    description = company_info.get("description", "")
    
    # STRICT: Require products OR services (not just industry from CSV)
    has_products_or_services = bool(products) or bool(services)
    
    # Also check description length - real analysis produces detailed descriptions
    has_detailed_description = len(description) > 100 if description else False
    
    # STRICT validation: Must have products/services from real company analysis
    if not has_products_or_services:
        logger.error(f"Missing REAL company analysis data for {request.companyName} - has_products_or_services=False")
        raise HTTPException(
            status_code=400,
            detail={
                "error": "Real company analysis data required",
                "message": "AEO mentions check requires REAL company analysis with products or services data. "
                          "Basic CSV data (industry only) is NOT sufficient for meaningful visibility scores. "
                          "You MUST run company analysis first (/company/analyze), then include the full result.",
                "validation": {
                    "products": len(products) if products else 0,
                    "services": len(services) if services else 0,
                    "industry": industry or "missing",
                    "description_length": len(description) if description else 0,
                },
                "requirement": "At least one product or service from company analysis is required.",
            }
        )
    
    logger.info(f"Starting mentions check for {request.companyName} (mode: {request.mode})")
    
    # Determine platforms to use based on mode
    if request.platforms:
        platforms = request.platforms
    elif request.mode == "fast":
        # Fast mode: only Gemini and ChatGPT (faster, cheaper)
        platforms = ["gemini", "chatgpt"]
    else:
        # Full mode: all platforms
        platforms = list(AI_PLATFORMS.keys())
    
    # Generate queries
    queries = generate_queries(
        request.companyName,
        request.companyAnalysis,
        request.numQueries,
        request.mode,
    )
    logger.info(f"Generated {len(queries)} queries")
    
    # Initialize stats
    platform_stats = {p: PlatformStats(mentions=0, quality_score=0, responses=0, errors=0) for p in platforms}
    dimension_stats: Dict[str, DimensionStats] = {}
    query_results: List[QueryResult] = []
    total_mentions = 0
    total_quality = 0.0
    total_tokens = 0
    total_cost = 0.0
    
    # Get competitors for mention detection
    competitors = []
    if request.companyAnalysis:
        competitors = request.companyAnalysis.competitors
    
    # Initialize dimension stats for all dimensions
    for query_data in queries:
        dimension = query_data["dimension"]
        if dimension not in dimension_stats:
            dimension_stats[dimension] = DimensionStats(mentions=0, quality_score=0, queries=0)
        dimension_stats[dimension].queries += 1
    
    # Process ALL queries in PARALLEL (much faster!)
    async def process_single_query(query_data):
        """Process a single query across all platforms."""
        query = query_data["query"]
        dimension = query_data["dimension"]
        logger.info(f"Querying: '{query}' ({dimension})")
        results = await query_all_platforms(query, platforms)
        return {"query_data": query_data, "results": results}
    
    logger.info(f"Processing {len(queries)} queries in parallel...")
    all_query_results = await asyncio.gather(
        *[process_single_query(q) for q in queries],
        return_exceptions=True
    )
    logger.info(f"All queries completed")
    
    # Process results from all parallel queries
    for query_result in all_query_results:
        if isinstance(query_result, Exception):
            logger.error(f"Query failed: {query_result}")
            continue
        
        query_data = query_result["query_data"]
        results = query_result["results"]
        query = query_data["query"]
        dimension = query_data["dimension"]
        
        for result in results:
            if isinstance(result, Exception):
                continue
            
            platform = result.get("platform", "unknown")
            
            if "error" in result:
                platform_stats[platform].errors += 1
                continue
            
            response_text = result.get("response", "")
            tokens = result.get("tokens", 0)
            cost = result.get("cost", 0.0)
            
            # Count mentions
            mention_data = count_mentions(response_text, request.companyName)
            
            # Extract competitor mentions
            comp_mentions = extract_competitor_mentions(response_text, competitors)
            
            # Create query result
            qr = QueryResult(
                query=query,
                dimension=dimension,
                platform=platform,
                raw_mentions=mention_data["raw_mentions"],
                capped_mentions=mention_data["capped_mentions"],
                quality_score=mention_data["quality_score"],
                mention_type=mention_data["mention_type"],
                position=mention_data["position"],
                competitor_mentions=comp_mentions,
                response_text=response_text[:500],  # Truncate for storage
            )
            query_results.append(qr)
            
            # Update stats
            platform_stats[platform].mentions += mention_data["capped_mentions"]
            platform_stats[platform].quality_score += mention_data["quality_score"]
            platform_stats[platform].responses += 1
            platform_stats[platform].total_tokens += tokens
            platform_stats[platform].cost += cost
            
            dimension_stats[dimension].mentions += mention_data["capped_mentions"]
            dimension_stats[dimension].quality_score += mention_data["quality_score"]
            
            total_mentions += mention_data["capped_mentions"]
            total_quality += mention_data["quality_score"]
            total_tokens += tokens
            total_cost += cost
    
    # Calculate visibility using presence-based formula
    # Visibility = "How often does AI mention this company?" (with quality boost)
    total_responses = sum(s.responses for s in platform_stats.values())
    max_quality = total_responses * 10.0
    
    # Count responses where company was actually mentioned
    responses_with_mentions = sum(1 for qr in query_results if qr.mention_type != 'none')
    
    # Presence rate is the primary factor (what % of queries mention the company?)
    presence_rate = responses_with_mentions / total_responses if total_responses > 0 else 0
    
    # Average quality when mentioned (how well are they mentioned?)
    avg_quality_when_mentioned = total_quality / max(responses_with_mentions, 1) if responses_with_mentions > 0 else 0
    
    # Quality factor: ranges from 0.85 (low quality mentions) to 1.15 (high quality mentions)
    # - Quality 0 → factor 0.85 (presence matters, but poor mentions hurt a bit)
    # - Quality 5 → factor 1.0 (neutral)
    # - Quality 10 → factor 1.15 (excellent mentions boost visibility)
    quality_factor = 0.85 + (avg_quality_when_mentioned / 10) * 0.30
    
    # Visibility = presence rate × quality factor (capped at 100%)
    visibility = min(100.0, presence_rate * quality_factor * 100)
    
    # Determine visibility band
    if visibility >= 80:
        band = "Dominant"
    elif visibility >= 60:
        band = "Strong"
    elif visibility >= 40:
        band = "Moderate"
    elif visibility >= 20:
        band = "Weak"
    else:
        band = "Minimal"
    
    # Calculate average quality scores
    for platform in platforms:
        if platform_stats[platform].responses > 0:
            platform_stats[platform].quality_score /= platform_stats[platform].responses
    
    for dimension in dimension_stats:
        if dimension_stats[dimension].queries > 0:
            dimension_stats[dimension].quality_score /= dimension_stats[dimension].queries
    
    execution_time = time.time() - start_time
    
    logger.info(f"Mentions check complete: visibility={visibility:.1f}% (presence={presence_rate*100:.1f}%), band={band}, mentions={total_mentions}")
    
    return MentionsCheckResponse(
        companyName=request.companyName,
        visibility=round(visibility, 1),
        band=band,
        mentions=total_mentions,
        presence_rate=round(presence_rate * 100, 1),  # As percentage (0-100)
        quality_score=round(avg_quality_when_mentioned, 2),  # Avg when mentioned (0-10)
        max_quality=max_quality,
        platform_stats=platform_stats,
        dimension_stats=dimension_stats,
        query_results=query_results,
        actualQueriesProcessed=len(queries),
        execution_time_seconds=round(execution_time, 2),
        total_cost=round(total_cost, 4),
        total_tokens=total_tokens,
        mode=request.mode,
    )


@app.get("/health")
async def health():
    """Service health check."""
    return {
        "status": "healthy",
        "service": "aeo-mentions-check",
        "version": "4.1.0",
        "backend": "scaile-services + DataForSEO SERP",
        "platforms": list(AI_PLATFORMS.keys()),
        "scaile_services_url": SCAILE_SERVICES_URL,
        "search_backend": "DataForSEO SERP via google_search tool",
        "chatgpt_model": "openai/gpt-4.1 via OpenAI",
    }


@app.get("/")
async def root():
    """Service info."""
    return {
        "service": "aeo-mentions-check",
        "version": "4.0.0",
        "endpoints": {
            "/check": "POST - Run mentions check",
            "/health": "GET - Service health",
        },
        "platforms": list(AI_PLATFORMS.keys()),
        "platform_details": {p: {"model": c["model"], "uses_dataforseo": c.get("needs_tool", False)} for p, c in AI_PLATFORMS.items()},
        "modes": {
            "fast": "10 queries, Gemini + ChatGPT only",
            "full": "50 queries, all 4 platforms (Perplexity, Claude, ChatGPT, Gemini)"
        },
        "search_backend": "DataForSEO SERP (via google_search tool)",
    }

